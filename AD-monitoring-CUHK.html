<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Malleable Convolution</title>
  <link rel="stylesheet" href="./AD-files/bootstrap.min.css">
  <link rel="stylesheet" href="./AD-files/temp.css">
  <link rel="stylesheet" href="./AD-files/MalleConv_style.css">
 <!-- <link href="https://www.google.com/favicon.ico" rel="icon"> -->
</head>

<body data-gr-c-s-loaded="true" data-new-gr-c-s-check-loaded="14.1043.0" data-gr-ext-installed="">

<div class="container">
  <table border="0" align="center">
    <tbody><tr>
      <td width="1500" height="200" align="center" valign="middle">
      <span class="title"><h1>Machine Learning Technologies for Advancing Digital Biomarkers for Alzheimer's Disease</h1></td>
    </tr>
       <tr>
        <td colspan="3" align="center"><h4> CUHK AIoT Lab</td> <!--  , &nbsp;  UT Austin </h4> </td></tr>
    </tr>
      <!--  
    <tr>
        <td colspan="3" align="center"><h5>
            <a href="https://yifanjiang.net/" target="_blank">Yifan Jiang<sup>1, 2</sup></a>, 
            <a href="https://bartwronski.com/" target="_blank">Bartlomiej Wronski<sup>1</sup></a>, 
            <a href="https://bmild.github.io/" target="_blank">Ben Mildenhall<sup>1</sup></a>, 
            <a href="https://jonbarron.info/" target="_blank">Jonathan T. Barron<sup>1</sup></a>, 
            <br>
            <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/" target="_blank">Zhangyang Wang<sup>2</sup></a>,
            <a href="https://people.csail.mit.edu/tfxue/" target="_blank">Tianfan Xue<sup>1</sup></a>
        </h5></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h4> <sup>1</sup> Google Research, &nbsp;  <sup>2</sup> UT Austin </h4> </td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h5> <a href="https://arxiv.org/pdf/2201.00392.pdf" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="./MalleConv_files/MalleConv.zip" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="./MalleConv_files/supplementary.pdf" target="_blank">[Supplementary Materials]</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </h5></td>
    </tr>
   -->
  </tbody></table>
 </div>
  
  <br>
  

<div class="text" style="text-align: left;">
      <h2>Overview</h2>
      <p>Alzheimer’s Disease (AD) and related dementia are a growing global health challenge due to the aging population. In this paper, we present ADMarker, the first end-to-end system that integrates multi-modal sensors and federated learning algo- rithms for detecting multidimensional AD digital biomarkers in natural living environments. ADmarker features a novel three-stage multi-modal federated learning architecture that can accurately detect digital biomarkers in a real-time and privacy-preserving manner. Our approach collectively ad- dresses several major real-world challenges, such as limited data labels, data heterogeneity, and limited computing re- sources. We built a compact multi-modality hardware system and deployed it in a four-week clinical trial involving 61 el- derly participants. The results indicate that ADMarker can accurately detect a comprehensive set of digital biomarkers with up to 95% accuracy and identify AD with an average of 87.5% accuracy. ADMarker offers a new platform that can allow AD clinicians to characterize and track the complex correlation between multidimensional interpretable digital biomarkers, demographic factors of patients, and AD diag- nosis in a longitudinal manner.</p>
  </div>
  
  <p><img src="./AD-files/ADMarker-overview.png" style="margin:auto;max-width:100%" align="middle"></p>

<br>

<div class="container">
  <h2>Funding</h2>
    <div class="overview">
    <p>
     <p> 1. Alzheimer’s Drug Discovery Foundation(ADDF), “Machine Learning Technologies for Advanced Digital Biomarkers for Alzheimer's Disease”, PI, HKD $5.6 million, 2021-2023.</p>
     <p> 2. Collaborative Research Fund, "Small Data Learning for Alzheimer's Disease: From Digital Biomarker to Personalized Intervention", PI, $8,230,720, 2022-2025.</p>
     <p> 3. RGC Research Grant General Research Fund, "HomeSense: A Pervasive System for Home Activity Recognition via Federated Learning", PI, $1,045,055, 2021-2023.</p>
    </p>
    </div>
</div>

<br> 
  
<div class="container">
  <h2>People</h2>
    <div class="overview">
    <p>
      <p> Professors: Guoliang Xing (PI, Professor, CUHK), Timothy Kwok (co-PI, Professor, CUHK), Doris S.F. Yu (co-PI, Professor, HKU), Allen Lee (co-PI, Assistant Professor, CUHK), Yuen-Yan Rosanna Chan (Assistant Professor, CUHK), Bolei Zhou (Assistant Professor, UCLA), Zhenyu Yan (Research Assistant Professor, CUHK)</p>
      <p> Students and Postdocs: Xiaomin Ouyang (Ph.D candidate, CUHK), Xian Shuai (Ph.D, CUHK), Yang Li (Ph.D student, CUHK), Li Pan (Research Assitant, CUHK), Xifan Zhang (Ph.D student, CUHK), Heming Fu (Research Assitant, CUHK), Sitong Cheng (Research Assitant, CUHK), Xinyan Wang (Undergraduate Student Helper, CUHK), Heming Fu (Research Assitant, CUHK), Shihua Cao (Postdoc Researcher, CUHK), Heming Fu (Research Assitant, CUHK)</p>
    </p>
    </div>
</div>
  
<br> 

<div class="text" style="text-align: left;">
      <h2>Patient Recruitment</h2>
      <p>Alzheimer’s Disease (AD) </p>
 </div>
  
<br>

<div class="container">
  <h2>Main publication</h2>
    <div class="overview">
    <p>
     <p> 1. Xiaomin Ouyang, Xian Shuai, Jiayu Zhou, Ivy Wang Shi, Zhiyuan Xie, Guoliang Xing, Jianwei Huang, "Cosmo: Contrastive Fusion Learning with Small Data for Multimodal Human Activity Recognition", The 28th Annual International Conference on Mobile Computing and Networking (MobiCom), 2022, acceptance ratio: 56/317=17.7%.</p>
     <p> 2. Xiaomin Ouyang, Zhiyuan Xie, Jiayu Zhou, Jianwei Huang, Guoliang Xing, "ClusterFL: A Similarity-Aware Federated Learning System for Human Activity Recognition", The 19th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys), 2021, acceptance ratio: 36/166=21.6%. </p>
     <p> 3. Linlin Tu, Xiaomin Ouyang, Jiayu Zhou, Yuze He, Guoliang Xing, "FedDL: Federated Learning via Dynamic Layer Sharing for Human Activity Recognition", The 19th ACM Conference on Embedded Networked Sensor Systems (SenSys), 2021, acceptance ratio: 25/139=17.98%. </p>
      <p> 4. Xian Shuai, Yulin Shen, Siyang Jiang, Zhihe Zhao, Zhenyu Yan, Guoliang Xing, "BalanceFL: Addressing Class Imbalance in Long-Tail Federated Learning", The 21st ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN), 2022, acceptance ratio: 38/126=30.2%.</p>
    </p>
    </div>
</div>

<!-- 
<div class="container">
    <h2>Method</h2>
      <img class="img_responsive" src="./MalleConv/arch_iared.png" alt="Teaser" style="margin:auto;max-width:80%;align=center">
      <div class="pipelines" style="text-align: left;">
        <p>
          Illustration of our IA-RED<sup>2</sup> framework. We divide the transformer into D groups. Each group contains a multi-head interpreter and L combinations of the MSA and FFN. Before input to the MSA and FFN, the patch tokens will be evaluated by the multi-head interpreter to drop some uninformative patches. The multi-head interpreters are optimized by reward considering both the efficiency and accuracy.
        </p>
      </div>
</div>
<br>
<div class="container">
  <h2>Qualitative Results of Interpretability-Aware Heatmaps</h2>
    <img class="img_responsive" src="./MalleConv/qual_heatmap.png" alt="Teaser" style="margin:auto;max-width:100%">
    <div class="pipelines" style="text-align: left;">
      <p>
        Qualitative result of the heatmaps which hightlight the informative region of the input images of <strong>MemNet</strong>, <strong>raw attention</strong> at the second block, and <strong>our method</strong> with DeiT-S model. We find that our method can obviously better interpret the part-level stuff of the objects of interest.
      </p>
    </div>
</div>
<br>
<div class="container">
  <h2>Qualitative Results of Redundancy Reduction</h2>
    <img class="img_responsive" src="./MalleConv/iared_red.png" alt="Teaser" style="margin:auto;max-width:100%">
    <div class="pipelines" style="text-align: left;">
      <p>
        We visualize the hierarchical redundancy reduction process of our method with the DeiT-S model. The number on the upper-left corner of each image indicates the ratio of the remaining patches. From left to right, we can see that the network drops the redundant patches and focuses more on the high-level features of the objects.
      </p>
    </div>
</div>
<br>
<div class="container">
  <h2>Reference</h2>
    <div class="overview">
    <p>
B. Pan and Y. Jiang and R. Panda and Z. Wang and R. Feris and A. Oliva. <strong>IA-RED<sup>2</sup>: Interpretability-Aware Redundancy Reduction for Vision Transformer.</strong> arXiv 2021 <a href="http://people.csail.mit.edu/bpan/ia-red/ia-red_files/ia_bib.txt">[Bibtex]</a>
<br>
</p></div></div><br>
<div class="container">
  <h2>Acknowledgements</h2>
    <div class="overview">
    <p>
      We thank IBM for the donation to MIT of the Satori GPU cluster. This work is supported by the MIT-IBM Watson AI Lab and its member companies, Nexplore and Woodside.
<br> -->


</p></div></div><br>

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
