<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>ADMarker CUHK</title>
  <link rel="stylesheet" href="./AD-files/bootstrap.min.css">
  <link rel="stylesheet" href="./AD-files/temp.css">
  <link rel="stylesheet" href="./AD-files/MalleConv_style.css">
  <link href="./images/CUHK.png" rel="icon">
</head>

<body data-gr-c-s-loaded="true" data-new-gr-c-s-check-loaded="14.1043.0" data-gr-ext-installed="">

<div class="container">
  <table border="0" align="center">
    <tbody><tr>
      <td width="1500" height="200" align="center" valign="middle">
      <span class="title"><h1>Machine Learning Technologies for Advancing Digital Biomarkers for Alzheimer's Disease</h1></td>
    </tr>
       <tr>
        <td colspan="2" align="center"><h4> CUHK AIoT Lab</td> <!--  , &nbsp;  UT Austin </h4> </td></tr>
    </tr>
      <!--  
    <tr>
        <td colspan="3" align="center"><h5>
            <a href="https://yifanjiang.net/" target="_blank">Yifan Jiang<sup>1, 2</sup></a>, 
            <a href="https://bartwronski.com/" target="_blank">Bartlomiej Wronski<sup>1</sup></a>, 
            <a href="https://bmild.github.io/" target="_blank">Ben Mildenhall<sup>1</sup></a>, 
            <a href="https://jonbarron.info/" target="_blank">Jonathan T. Barron<sup>1</sup></a>, 
            <br>
            <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/" target="_blank">Zhangyang Wang<sup>2</sup></a>,
            <a href="https://people.csail.mit.edu/tfxue/" target="_blank">Tianfan Xue<sup>1</sup></a>
        </h5></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h4> <sup>1</sup> Google Research, &nbsp;  <sup>2</sup> UT Austin </h4> </td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h5> <a href="https://arxiv.org/pdf/2201.00392.pdf" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="./MalleConv_files/MalleConv.zip" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="./MalleConv_files/supplementary.pdf" target="_blank">[Supplementary Materials]</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </h5></td>
    </tr>
   -->
  </tbody></table>
  
  <br>
  
  <p><img src="./AD-files/ADMarker-overview.png" style="margin:auto;max-width:90%" align="middle"></p>
  
  <div class="text" style="text-align: left;">
      <h2>Overview</h2>
      <p>Alzheimer’s Disease (AD) and related dementia are a growing global health challenge due to the aging population. A major barrier to the treatment of AD is that many patients are either not diagnosed or diagnosed at the late stages of the disease. A recent major advance in early AD diagnosis and intervention is to leverage AI and sensor devices to capture physiological, behavioral, and lifestyle symptoms of AD (e.g., activities of daily living and social interactions) in natural home environments, referred to as <b>digital biomarkers</b>. In this project, we propose the first end-to-end system that integrates multi-modal sensors and federated learning algorithms for detecting multidimensional AD digital biomarkers in natural living environments. We developed a compact multi-modality hardware system that can function for up to months in home environments to detect digital biomarkers of AD. It incorporates three privacy-preserving sensors (a depth camera, a mmWave radar, and a microphone), an NVIDIA single-board edge computer, and a 4G cellular interface that can communicate with the server. On top of the hardware system, we design a multi-modal federated learning system that can accurately detect more than 20 digital biomarkers in a real-time and privacy-preserving manner. Our approach collectively addresses several major real-world challenges, such as limited data labels, data heterogeneity, and limited computing resources. </p>
    <!--We address several practical challenges to make the hardware system durable, power-efficient, and privacy-preserving.-->
    <p> <b>Patient Recruitment and Results</b>: To date, our system has been deployed it in a four-week clinical trial involving 61 elderly participants (29 females and 32 males, 62 - 90 years old). The participants were from three groups: 18 with Alzheimer’s Disease, 16 with mild cognitive impairment (MCI), and 27 are cognitively normal. The results indicate that our system can accurately detect a comprehensive set of digital biomarkers with up to 95% accuracy and identify AD with an average of 87.5% accuracy. Our system offers a new platform that can allow AD clinicians to characterize and track the complex correlation between multidimensional interpretable digital biomarkers, demographic factors of patients, and AD diagnosis in a longitudinal manner.  </p>
    
    <p> <b>Ethics</b>: All the data collection in this study was approved by the Institutional Review Board of CUHK, and Clinical Research Ethics Committee of Joint CUHK and Hong Kong Hospital Authority (New Territories East Cluster).</p>
  </div>
  
 </div>

<br>

<div class="container">
  <h2>People</h2>
    <div class="overview">
    <p>
      <p> <li> Professors: Guoliang Xing (PI, Professor, CUHK), Timothy Kwok (co-PI, Professor, CUHK), Doris S.F. Yu (co-PI, Professor, HKU), Allen Lee (co-PI, Assistant Professor, CUHK), Yuen-Yan Rosanna Chan (Assistant Professor, CUHK), Bolei Zhou (Assistant Professor, UCLA), Zhenyu Yan (Research Assistant Professor, CUHK) </li></p>
      <p> <li> Students and Postdocs: Xiaomin Ouyang (Ph.D candidate, CUHK), Xian Shuai (Ph.D, CUHK), Yang Li (Ph.D student, CUHK), Li Pan (Research Assitant, CUHK), Xifan Zhang (Ph.D student, CUHK), Heming Fu (Research Assitant, CUHK), Sitong Cheng (Research Assitant, CUHK), Xinyan Wang (Undergraduate Student Helper, CUHK), Heming Fu (Research Assitant, CUHK), Shihua Cao (Postdoc Researcher, CUHK), Hazel Mok (Postdoc Researcher, CUHK) </li></p>
    </p>
    </div>
</div>
  
<br> 

 <!-- 
<div class="container">
<div class="text" style="text-align: left;">
      <h2>Patient Recruitment</h2>
      <p>Alzheimer’s Disease (AD) </p>
 </div>
   </div>
-->

<div class="container">
  <h2>Main Publications</h2>
    <div class="overview">
    <p>
     <p> 1. Xiaomin Ouyang, Xian Shuai, Jiayu Zhou, Ivy Wang Shi, Zhiyuan Xie, Guoliang Xing, Jianwei Huang, "Cosmo: Contrastive Fusion Learning with Small Data for Multimodal Human Activity Recognition", The 28th Annual International Conference on Mobile Computing and Networking (MobiCom), 2022, acceptance ratio: 56/317=17.7%.</p>
     <p> 2. Xiaomin Ouyang, Zhiyuan Xie, Jiayu Zhou, Jianwei Huang, Guoliang Xing, "ClusterFL: A Similarity-Aware Federated Learning System for Human Activity Recognition", The 19th Annual International Conference on Mobile Systems, Applications, and Services (MobiSys), 2021, acceptance ratio: 36/166=21.6%. </p>
     <p> 3. Linlin Tu, Xiaomin Ouyang, Jiayu Zhou, Yuze He, Guoliang Xing, "FedDL: Federated Learning via Dynamic Layer Sharing for Human Activity Recognition", The 19th ACM Conference on Embedded Networked Sensor Systems (SenSys), 2021, acceptance ratio: 25/139=17.98%. </p>
      <p> 4. Xian Shuai, Yulin Shen, Siyang Jiang, Zhihe Zhao, Zhenyu Yan, Guoliang Xing, "BalanceFL: Addressing Class Imbalance in Long-Tail Federated Learning", The 21st ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN), 2022, acceptance ratio: 38/126=30.2%.</p>
    </p>
    </div>
</div>

<br> 

<div class="container">
  <h2>Funding</h2>
    <div class="overview">
    <p>
     <p> 1. Alzheimer’s Drug Discovery Foundation(ADDF), “Machine Learning Technologies for Advanced Digital Biomarkers for Alzheimer's Disease”, PI, HKD $5,560,354, 2021-2023.</p>
     <p> 2. Collaborative Research Fund, "Small Data Learning for Alzheimer's Disease: From Digital Biomarker to Personalized Intervention", PI, HKD $8,230,720, 2022-2025.</p>
     <p> 3. RGC Research Grant General Research Fund, "HomeSense: A Pervasive System for Home Activity Recognition via Federated Learning", PI, HKD $1,045,055, 2021-2023.</p>
    </p>
    </div>
</div>

<!-- 
<div class="container">
    <h2>Method</h2>
      <img class="img_responsive" src="./MalleConv/arch_iared.png" alt="Teaser" style="margin:auto;max-width:80%;align=center">
      <div class="pipelines" style="text-align: left;">
        <p>
          Illustration of our IA-RED<sup>2</sup> framework. We divide the transformer into D groups. Each group contains a multi-head interpreter and L combinations of the MSA and FFN. Before input to the MSA and FFN, the patch tokens will be evaluated by the multi-head interpreter to drop some uninformative patches. The multi-head interpreters are optimized by reward considering both the efficiency and accuracy.
        </p>
      </div>
</div>
<br>
<div class="container">
  <h2>Qualitative Results of Interpretability-Aware Heatmaps</h2>
    <img class="img_responsive" src="./MalleConv/qual_heatmap.png" alt="Teaser" style="margin:auto;max-width:100%">
    <div class="pipelines" style="text-align: left;">
      <p>
        Qualitative result of the heatmaps which hightlight the informative region of the input images of <strong>MemNet</strong>, <strong>raw attention</strong> at the second block, and <strong>our method</strong> with DeiT-S model. We find that our method can obviously better interpret the part-level stuff of the objects of interest.
      </p>
    </div>
</div>
<br>
<div class="container">
  <h2>Qualitative Results of Redundancy Reduction</h2>
    <img class="img_responsive" src="./MalleConv/iared_red.png" alt="Teaser" style="margin:auto;max-width:100%">
    <div class="pipelines" style="text-align: left;">
      <p>
        We visualize the hierarchical redundancy reduction process of our method with the DeiT-S model. The number on the upper-left corner of each image indicates the ratio of the remaining patches. From left to right, we can see that the network drops the redundant patches and focuses more on the high-level features of the objects.
      </p>
    </div>
</div>
<br>
<div class="container">
  <h2>Reference</h2>
    <div class="overview">
    <p>
B. Pan and Y. Jiang and R. Panda and Z. Wang and R. Feris and A. Oliva. <strong>IA-RED<sup>2</sup>: Interpretability-Aware Redundancy Reduction for Vision Transformer.</strong> arXiv 2021 <a href="http://people.csail.mit.edu/bpan/ia-red/ia-red_files/ia_bib.txt">[Bibtex]</a>
<br>
</p></div></div><br>
<div class="container">
  <h2>Acknowledgements</h2>
    <div class="overview">
    <p>
      We thank IBM for the donation to MIT of the Satori GPU cluster. This work is supported by the MIT-IBM Watson AI Lab and its member companies, Nexplore and Woodside.
<br> -->


</p></div></div><br>

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
