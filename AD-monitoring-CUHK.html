<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Malleable Convolution</title>
  <link rel="stylesheet" href="./bootstrap.min.css">
  <link rel="stylesheet" href="./temp.css">
  <link rel="stylesheet" href="./MalleConv_style.css">
  <link href="https://www.google.com/favicon.ico" rel="icon">
</head>

<body data-gr-c-s-loaded="true" data-new-gr-c-s-check-loaded="14.1043.0" data-gr-ext-installed="">

<div class="container">
  <table border="0" align="center">
    <tbody><tr>
      <td width="1500" height="200" align="center" valign="middle">
      <span class="title"><h1>Fast and High-quality Image Denoising via <br> Malleable Convolutions</h1></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h5>
            <a href="https://yifanjiang.net/" target="_blank">Yifan Jiang<sup>1, 2</sup></a>, 
            <a href="https://bartwronski.com/" target="_blank">Bartlomiej Wronski<sup>1</sup></a>, 
            <a href="https://bmild.github.io/" target="_blank">Ben Mildenhall<sup>1</sup></a>, 
            <a href="https://jonbarron.info/" target="_blank">Jonathan T. Barron<sup>1</sup></a>, 
            <br>
            <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/" target="_blank">Zhangyang Wang<sup>2</sup></a>,
            <a href="https://people.csail.mit.edu/tfxue/" target="_blank">Tianfan Xue<sup>1</sup></a>
        </h5></td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h4> <sup>1</sup> Google Research, &nbsp;  <sup>2</sup> UT Austin </h4> </td>
    </tr>
    <tr>
        <td colspan="3" align="center"><h5> <a href="https://arxiv.org/pdf/2201.00392.pdf" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="./MalleConv_files/MalleConv.zip" target="_blank">[Code]</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <a href="./MalleConv_files/supplementary.pdf" target="_blank">[Supplementary Materials]</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; </h5></td>
    </tr>
  </tbody></table>
  <br>
  <p><img src="./MalleConv_files/difference.png" style="margin:auto;max-width:100%" align="middle"></p>
  <div class="text" style="text-align: left;">
      <h2>Abstract</h2>
      <p>Many image processing networks apply a single set of static convolutional kernels across the entire input image, which is sub-optimal for natural images, as they often consist of heterogeneous visual patterns. Recent works in classification, segmentation, and image restoration have demonstrated that dynamic kernels outperform static kernels at modeling local image statistics. However, these works often adopt per-pixel convolution kernels, which introduce high memory and computation costs. To achieve spatial-varying processing without significant overhead, we present Malleable Convolution (MalleConv), as an efficient variant of dynamic convolution. The weights of MalleConv are dynamically produced by an efficient predictor network capable of generating content-dependent outputs at specific spatial locations. Unlike previous works, MalleConv generates a much smaller set of spatially-varying kernels from input, which enlarges the network's receptive field and significantly reduces computational and memory costs. These kernels are then applied to a full-resolution feature map through an efficient slice-and-conv operator with minimum memory overhead. We further build an efficient denoising network using MalleConv, coined as MalleNet. It achieves high quality results without very deep architecture, e.g., reaching 8.91x faster speed compared to the best performed denoising algorithms (SwinIR), while maintaining similar performance. We also show that a single MalleConv added to a standard convolution-based backbone can contribute significantly to reducing the computational cost or boosting image quality at a similar cost</p>
  </div>
</div>

<br>

<!-- <div class="container">
  <h2>Main Idea</h2>
    <div class="overview">
    <p>
      Our main goal is to automatically decide which feature maps to compute for each input video in order to classify it correctly with the minimum computation. The intuition behind our proposed method is that there are many similar feature maps along the temporal and channel dimensions. For each video instance, we estimate the ratio of feature maps that need to be fully computed along the temporal dimension and channel dimension. Then, for the other feature maps, <strong>reconstruct them from those pre-computed feature maps using cheap linear operations</strong>.
    </p>
    </div>
</div> -->

<!-- <br> -->
<!-- 
<div class="container">
    <h2>Method</h2>
      <img class="img_responsive" src="./MalleConv/arch_iared.png" alt="Teaser" style="margin:auto;max-width:80%;align=center">
      <div class="pipelines" style="text-align: left;">
        <p>
          Illustration of our IA-RED<sup>2</sup> framework. We divide the transformer into D groups. Each group contains a multi-head interpreter and L combinations of the MSA and FFN. Before input to the MSA and FFN, the patch tokens will be evaluated by the multi-head interpreter to drop some uninformative patches. The multi-head interpreters are optimized by reward considering both the efficiency and accuracy.
        </p>
      </div>
</div>
<br>
<div class="container">
  <h2>Qualitative Results of Interpretability-Aware Heatmaps</h2>
    <img class="img_responsive" src="./MalleConv/qual_heatmap.png" alt="Teaser" style="margin:auto;max-width:100%">
    <div class="pipelines" style="text-align: left;">
      <p>
        Qualitative result of the heatmaps which hightlight the informative region of the input images of <strong>MemNet</strong>, <strong>raw attention</strong> at the second block, and <strong>our method</strong> with DeiT-S model. We find that our method can obviously better interpret the part-level stuff of the objects of interest.
      </p>
    </div>
</div>
<br>
<div class="container">
  <h2>Qualitative Results of Redundancy Reduction</h2>
    <img class="img_responsive" src="./MalleConv/iared_red.png" alt="Teaser" style="margin:auto;max-width:100%">
    <div class="pipelines" style="text-align: left;">
      <p>
        We visualize the hierarchical redundancy reduction process of our method with the DeiT-S model. The number on the upper-left corner of each image indicates the ratio of the remaining patches. From left to right, we can see that the network drops the redundant patches and focuses more on the high-level features of the objects.
      </p>
    </div>
</div>
<br>
<div class="container">
  <h2>Reference</h2>
    <div class="overview">
    <p>
B. Pan and Y. Jiang and R. Panda and Z. Wang and R. Feris and A. Oliva. <strong>IA-RED<sup>2</sup>: Interpretability-Aware Redundancy Reduction for Vision Transformer.</strong> arXiv 2021 <a href="http://people.csail.mit.edu/bpan/ia-red/ia-red_files/ia_bib.txt">[Bibtex]</a>
<br>
</p></div></div><br>
<div class="container">
  <h2>Acknowledgements</h2>
    <div class="overview">
    <p>
      We thank IBM for the donation to MIT of the Satori GPU cluster. This work is supported by the MIT-IBM Watson AI Lab and its member companies, Nexplore and Woodside.
<br> -->


</p></div></div><br>

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
